# /usr/share/logstash/pipeline/logstash.conf

input {
  # 输入数据源为filebeat
  beats {
    port => "5044"
  }
}

filter {
  # 定义日志数据格式,可在kibana中先调试格式
  grok {
    match => { "message" => "%{DATA:logTime} \[%{DATA:hostName}\] \[%{DATA:appName}\] \[%{DATA:logLevel}\] \[%{DATA:threadName}\] \[%{DATA:traceId}\] \[%{DATA:className}\] : %{GREEDYDATA:logMsg}"}
  }
  # 定义时间戳的格式，配置kibana保证时区展示正确
  date {
    match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
    target=>"@timestamp"
    timezone=>"Asia/Shanghai"
  }
  mutate { 
    remove_field => ["input","source","beat","tags","host","log","prospector"]
  }
}

output {
  # 输出到elasticsearch
  elasticsearch {
    hosts => ["192.168.99.100:9200"]
    index => "logging-coding-%{+YYYY.MM.dd}"
    document_type => "logging-coding"
  }
  
  # 输出到stdout
  stdout{
    codec => "rubydebug"
  }
}
